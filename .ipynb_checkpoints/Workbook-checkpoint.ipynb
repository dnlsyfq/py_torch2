{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensor 1D , Vector\n",
    "* Tensor 2D , Matrix\n",
    "* Tensor 3D , i.e RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Activation fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Layer Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4034)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation(torch.sum(weights*features + bias))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Layer Multiplication Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(features.size())\n",
    "print(weights.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation(torch.mm(features,weights.reshape(5,1)) + bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi Layer Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3171]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation(torch.mm(features,W1) + B1)\n",
    "y1 = activation(torch.mm(y,W2) + B2)\n",
    "y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Numpy & Tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03063967, 0.69532731, 0.13456494],\n",
       "       [0.43627969, 0.78637439, 0.72992679],\n",
       "       [0.42196099, 0.77819349, 0.12641713],\n",
       "       [0.24377631, 0.96663315, 0.18739733]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3866, 0.4842, 0.1697],\n",
       "        [0.2559, 0.0994, 0.8159],\n",
       "        [0.3273, 0.4472, 0.1200],\n",
       "        [0.8456, 0.2272, 0.8356]], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.random.rand(4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45777364, 0.07946364, 0.91131455],\n",
       "       [0.71851395, 0.02886516, 0.55277632],\n",
       "       [0.40073102, 0.11895671, 0.77683529],\n",
       "       [0.90232409, 0.48395095, 0.26516869]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.random.rand(4,3)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Networks with Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import helper\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /Users/danialsyafiq/.pytorch/MNIST_data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "img,lab = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape) # 64 img 1 color 28 width 28 height "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(lab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x129385290>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img[1].numpy().squeeze(),cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = img.reshape(img.shape[0],-1) # 64,784 , keep no. of batches the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = activation(torch.mm(inputs,w1) + b1)\n",
    "out = torch.mm(y,w2) + b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "probabilities = softmax(out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with a softmax activation as shown above. You can use a ReLU activation with the nn.ReLU module or F.relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the layers, 128, 64, 10 units each\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
